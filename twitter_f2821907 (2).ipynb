{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of MEPs' Tweets\n",
    "Erasmia Kornelatou, f2821907"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\astar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\astar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Downloading the stop words list\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "* Get the dataset from <https://www.clarin.si/repository/xmlui/handle/11356/1071>.\n",
    "\n",
    "* You will use the `retweets.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the retweets.csv\n",
    "data = pd.read_csv('...retweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Keep only the records for which the language of the original tweet is in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['lang'] == 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get the text of the *original tweet* and add it to the dataset as an extra column. Use the Tweeter API to get the text (e.g., with Tweepy). In order not to run into rate limits you can ask for multiple tweets with one call.\n",
    "* Keep only the records for which you were able to download the tweet text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We convert origin ids of tweets to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids =data['origTweetId'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We connect to twitter with credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tw.OAuthHandler('...', '...')\n",
    "auth.set_access_token('...', '...')\n",
    "api = tw.API(auth,wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We load 100 tweets per request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of ids\n",
    "num = len(ids)\n",
    "# limit for forloop\n",
    "limit = int(num / 100) + 1\n",
    "# tweets loaded by twitter\n",
    "tweets = []\n",
    "    \n",
    " #loading 100 tweets per request   \n",
    "for i in range(limit):\n",
    "    temp = (i + 1) * 100\n",
    "    if(num < temp):\n",
    "        tweets.extend(api.statuses_lookup(id_ = ids[i * 100:num]))\n",
    "    else:\n",
    "        tweets.extend(api.statuses_lookup(id_ = ids[i * 100:temp]))\n",
    "        \n",
    "tweets  \n",
    "tweet_list = []\n",
    "for tweet in tweets:\n",
    "    tweet_list.append(tweet._json)\n",
    "data2 = pd.DataFrame(tweet_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We add the text in a new column in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_array = pd.merge(left = data,right = data2[['id','text']],left_on= 'origTweetId',right_on= 'id',how ='inner').drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Group the records by the European group of the MEP that posted the original tweet. If you see that there are groups with very few tweets (less than 50), drop them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origUserId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>origMepGroupShort</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALDE</th>\n",
       "      <td>1868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ECR</th>\n",
       "      <td>1102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EFDD</th>\n",
       "      <td>3283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENL</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EPP</th>\n",
       "      <td>2112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GUE-NGL</th>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Greens-EFA</th>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NI</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S&amp;D</th>\n",
       "      <td>3107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   origUserId\n",
       "origMepGroupShort            \n",
       "ALDE                     1868\n",
       "ECR                      1102\n",
       "EFDD                     3283\n",
       "ENL                        23\n",
       "EPP                      2112\n",
       "GUE-NGL                   356\n",
       "Greens-EFA               1199\n",
       "NI                          1\n",
       "S&D                      3107"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eurogroup =pd.DataFrame(tw_array['origUserId'].groupby(tw_array['origMepGroupShort']).count())\n",
    "eurogroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it seems, NI and ENL  are groups with very few tweets (less than 50), so we are going to drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_array = tw_array[tw_array['origMepGroupShort'] != 'ENL']\n",
    "tw_array = tw_array[tw_array['origMepGroupShort'] != 'NI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "* You may want to strip accents, convert everything to lowercase, and remove all English stopwords. In general, you may experiment with additional ideas about how best to tokenize etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We remove duplicate rows as they contain the same tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_array = tw_array.drop_duplicates(subset='text', keep=\"last\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We remove URLS as they do not help in classification, repeated characters (ex. likeeeeeeeeee into like) and accents (ex. café into cafe) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tw_array['text']:\n",
    "    t = re.sub('((@\\S+|https?://\\S+))', 'URL', t) \n",
    "    t = word_tokenize(t) \n",
    "    for word in t:\n",
    "       word = unicodedata.normalize('NFKD', word).encode('ASCII', 'ignore')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We remove symbols: #.,:;/\\()'|+“%‘’*&€@'?!-  ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = list(\"#.,:;/\\()'|+“%‘’*&€@'?!-\")\n",
    "for s in symbols:\n",
    "    tw_array['text'] = tw_array['text'].str.replace(s, '')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We convert everything to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_array['text'] = tw_array['text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We remove Possessive pronouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_array['text'] = tw_array['text'].str.replace(\"'s\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. We remove \\r , \\n and \"  ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_array['text'] = tw_array['text'].str.replace(\"\\r\", \" \")\n",
    "tw_array['text'] = tw_array['text'].str.replace(\"\\n\", \" \")\n",
    "tw_array['text'] = tw_array['text'].str.replace('\"', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. We remove stop words in english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(stopwords.words('english'))\n",
    "for stop in stop_words:\n",
    "\n",
    "    regex = r\"\\b\" + stop + r\"\\b\"\n",
    "    tw_array['text'] = tw_array['text'].str.replace(regex, '')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. We remove double spaces.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove double spaces    \n",
    "tw_array['text'] = tw_array['text'].str.replace(\"  \", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. We remove null tweet texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_array = tw_array[tw_array.text.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "* Train at least two algorithms to learn to classify an unseen tweet. The target variable should be the political party of the original poster and the training features should be the original tweet's text.\n",
    "\n",
    "* You should split your data to training and testing datasets, try the different algorithms with cross validation on the training dataset, and find the best hyperparameters for the best algorithm. \n",
    " \n",
    "* The tweet texts must be converted to a format suitable the classification, bag of word matrices or tf-idf matrices. You must investigate which one gives the best results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tw_array['text'],tw_array['origMepGroupId'], test_size=0.15, random_state=42,shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a. Multinomial Naive Bayes with tf-idf matrices format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5492772667542707"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(strip_accents = 'unicode',lowercase =True,stop_words ='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "predicted = text_clf.predict(X_test)\n",
    "print(\"The accuracy is: \")\n",
    "np.mean(predicted == y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1b. Multinomial Naive Bayes with bag of word matrices format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6254927726675427"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(strip_accents = 'unicode',lowercase =True,stop_words ='english')),\n",
    "                     ('clf', MultinomialNB())])\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "predicted = text_clf.predict(X_test)\n",
    "print(\"The accuracy is: \")\n",
    "np.mean(predicted == y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a. Stochastic Gradient Descent with tf-idf matrices format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.621550591327201"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([('vect', CountVectorizer(strip_accents = 'unicode',lowercase =True,stop_words ='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3,\n",
    "                                           random_state=42))])\n",
    "text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "print(\"The accuracy is: \")\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b. Stochastic Gradient Descent with bag of word matrices format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6353482260183968"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(strip_accents = 'unicode',lowercase =True,stop_words ='english')),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3,\n",
    "                                           random_state=42))])\n",
    "text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "print(\"The accuracy is: \")\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3a. Support Vector Machines Classifier with tf-idf matrices format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6360052562417872"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(strip_accents = 'unicode',lowercase =True,stop_words ='english')),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                     ('clf',SVC(C=1, kernel='linear'))])                                           \n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "predicted_svc = text_clf.predict(X_test)\n",
    "print(\"The accuracy is: \")\n",
    "np.mean(predicted_svc == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3b. Support Vector Machines Classifier with  bag of word matrices format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6090670170827858"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "text_clf = Pipeline([('vect', CountVectorizer(strip_accents = 'unicode',lowercase =True,stop_words ='english')),\n",
    "                     ('clf',SVC(C=1, kernel='linear'))])                                           \n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "predicted_svc = text_clf.predict(X_test)\n",
    "print(\"The accuracy is: \")\n",
    "np.mean(predicted_svc == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To gauge the efficacy of the algorithm, report also the results of a baseline classifier, using, for instance, scikit-learn's [`DummyClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.238391008577344"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(tw_array['text'], tw_array['origMepGroupId'])\n",
    "dummy_clf.predict(tw_array['text'])\n",
    "dummy_clf.score(tw_array['text'], tw_array['origMepGroupId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we choose the Support Vector Machines Classifier with tf-idf matrices format format as it has the highest accuracy  (0.6360052562417872) , so it is greater than 0.238391008577344 which is the accuracy for the dummyClassifier. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
